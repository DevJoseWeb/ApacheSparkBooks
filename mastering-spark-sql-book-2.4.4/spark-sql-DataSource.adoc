== [[DataSource]] DataSource -- Pluggable Data Provider Framework

`DataSource` is one of the main parts of *Data Source API* in Spark SQL (together with link:spark-sql-DataFrameReader.adoc[DataFrameReader] for loading datasets, link:spark-sql-DataFrameWriter.adoc[DataFrameWriter] for saving datasets and `StreamSourceProvider` for creating streaming sources).

`DataSource` models a *pluggable data provider framework* with the <<providers, extension points>> for Spark SQL integrators to expand the list of supported external data sources in Spark SQL.

`DataSource` takes a list of <<paths, file system paths that hold data>>. The list is empty by default, but can be different per data source:

* The <<spark-sql-CatalogTable.adoc#location, location URI>> of a <<spark-sql-LogicalPlan-HiveTableRelation.adoc#, HiveTableRelation>> (when `HiveMetastoreCatalog` is requested to <<spark-sql-HiveMetastoreCatalog.adoc#convertToLogicalRelation, convert a HiveTableRelation to a LogicalRelation>>)

* The table name of a <<spark-sql-LogicalPlan-UnresolvedRelation.adoc#, UnresolvedRelation>> (when <<spark-sql-Analyzer-ResolveSQLOnFile.adoc#, ResolveSQLOnFile>> logical evaluation rule is executed)

* The files in a directory when Spark Structured Streaming's `FileStreamSource` is requested for batches

`DataSource` is <<creating-instance, created>> when:

* `DataFrameWriter` is requested to link:spark-sql-DataFrameWriter.adoc#saveToV1Source[save to a data source (per Data Source V1 contract)]

* link:spark-sql-Analyzer-FindDataSourceTable.adoc#readDataSourceTable[FindDataSourceTable] and link:spark-sql-Analyzer-ResolveSQLOnFile.adoc#apply[ResolveSQLOnFile] logical evaluation rules are executed

* link:spark-sql-LogicalPlan-CreateDataSourceTableCommand.adoc#run[CreateDataSourceTableCommand], link:spark-sql-LogicalPlan-CreateDataSourceTableAsSelectCommand.adoc#run[CreateDataSourceTableAsSelectCommand], link:spark-sql-LogicalPlan-InsertIntoDataSourceDirCommand.adoc#run[InsertIntoDataSourceDirCommand], link:spark-sql-LogicalPlan-CreateTempViewUsing.adoc#run[CreateTempViewUsing] are executed

* `HiveMetastoreCatalog` is requested to link:spark-sql-HiveMetastoreCatalog.adoc#convertToLogicalRelation[convertToLogicalRelation]

* Spark Structured Streaming's `FileStreamSource`, `DataStreamReader` and `DataStreamWriter`

[[providers]]
.DataSource's Provider (and Format) Contracts
[cols="1,3",options="header",width="100%"]
|===
| Extension Point
| Description

| link:spark-sql-CreatableRelationProvider.adoc[CreatableRelationProvider]
| [[CreatableRelationProvider]] Data source that saves the result of a structured query per save mode and returns the schema

| link:spark-sql-FileFormat.adoc[FileFormat]
a| [[FileFormat]] Used in:

* <<sourceSchema, sourceSchema>> for streamed reading

* <<write, write>> for writing a `DataFrame` to a `DataSource` (as part of creating a table as select)

| link:spark-sql-RelationProvider.adoc[RelationProvider]
| [[RelationProvider]] Data source that supports schema inference and can be accessed using SQL's `USING` clause

| link:spark-sql-SchemaRelationProvider.adoc[SchemaRelationProvider]
| [[SchemaRelationProvider]] Data source that requires a user-defined schema

| `StreamSourceProvider`
a| [[StreamSourceProvider]] Used in:

* <<sourceSchema, sourceSchema>> and <<createSource, createSource>> for streamed reading

* <<createSink, createSink>> for streamed writing

* <<resolveRelation, resolveRelation>> for resolved link:spark-sql-BaseRelation.adoc[BaseRelation].
|===

As a user, you interact with `DataSource` by link:spark-sql-DataFrameReader.adoc[DataFrameReader] (when you execute link:spark-sql-SparkSession.adoc#read[spark.read] or link:spark-sql-SparkSession.adoc#readStream[spark.readStream]) or SQL's `CREATE TABLE USING`.

[source, scala]
----
// Batch reading
val people: DataFrame = spark.read
  .format("csv")
  .load("people.csv")

// Streamed reading
val messages: DataFrame = spark.readStream
  .format("kafka")
  .option("subscribe", "topic")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .load
----

`DataSource` uses a link:spark-sql-SparkSession.adoc[SparkSession], a class name, a collection of `paths`, optional user-specified link:spark-sql-schema.adoc[schema], a collection of partition columns, a bucket specification, and configuration options.

NOTE: Data source is also called a *table provider*.

When requested to <<resolveRelation, resolve a batch (non-streaming) FileFormat>>, `DataSource` creates a <<spark-sql-BaseRelation-HadoopFsRelation.adoc#, HadoopFsRelation>> with the optional <<bucketSpec, bucketing specification>>.

[[internal-registries]]
.DataSource's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| `providingClass`
| [[providingClass]] The Java class (`java.lang.Class`) that...FIXME

Used when...FIXME

| `sourceInfo`
| [[sourceInfo]] `SourceInfo`

Used when...FIXME

| `caseInsensitiveOptions`
| [[caseInsensitiveOptions]] FIXME

Used when...FIXME

| `equality`
| [[equality]] FIXME

Used when...FIXME

| `backwardCompatibilityMap`
| [[backwardCompatibilityMap]] FIXME

Used when...FIXME
|===

=== [[writeAndRead]] Writing Data to Data Source per Save Mode Followed by Reading Rows Back (as BaseRelation) -- `writeAndRead` Method

[source, scala]
----
writeAndRead(mode: SaveMode, data: DataFrame): BaseRelation
----

CAUTION: FIXME

NOTE: `writeAndRead` is used exclusively when link:spark-sql-LogicalPlan-CreateDataSourceTableAsSelectCommand.adoc#run[CreateDataSourceTableAsSelectCommand] logical command is executed.

=== [[write]] Writing DataFrame to Data Source Per Save Mode -- `write` Method

[source, scala]
----
write(mode: SaveMode, data: DataFrame): BaseRelation
----

`write` writes the result of executing a structured query (as link:spark-sql-DataFrame.adoc[DataFrame]) to a data source per save `mode`.

Internally, `write` <<lookupDataSource, looks up the data source>> and branches off per <<providingClass, providingClass>>.

[[write-providingClass-branches]]
.write's Branches per Supported providingClass (in execution order)
[width="100%",cols="1,2",options="header"]
|===
| providingClass
| Description

| link:spark-sql-CreatableRelationProvider.adoc[CreatableRelationProvider]
| Executes link:spark-sql-CreatableRelationProvider.adoc#createRelation[CreatableRelationProvider.createRelation]

| link:spark-sql-FileFormat.adoc[FileFormat]
| <<writeInFileFormat, writeInFileFormat>>

| _others_
| Reports a `RuntimeException`
|===

NOTE: `write` does not support the internal `CalendarIntervalType` in the link:spark-sql-schema.adoc[schema of `data` `DataFrame`] and throws a `AnalysisException` when there is one.

NOTE: `write` is used exclusively when link:spark-sql-LogicalPlan-RunnableCommand.adoc#SaveIntoDataSourceCommand[SaveIntoDataSourceCommand] is executed.

=== [[writeInFileFormat]] `writeInFileFormat` Internal Method

CAUTION: FIXME

For link:spark-sql-FileFormat.adoc[FileFormat] data sources, `write` takes all `paths` and `path` option and makes sure that there is only one.

NOTE: `write` uses Hadoop's https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/Path.html[Path] to access the https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html[FileSystem] and calculate the qualified output path.

`write` requests `PartitioningUtils` to link:spark-sql-PartitioningUtils.adoc#validatePartitionColumn[validatePartitionColumn].

When appending to a table, ...FIXME

In the end, `write` (for a link:spark-sql-FileFormat.adoc[FileFormat] data source) link:spark-sql-SessionState.adoc#executePlan[prepares a `InsertIntoHadoopFsRelationCommand` logical plan] with link:spark-sql-QueryExecution.adoc#toRdd[executes] it.

CAUTION: FIXME Is `toRdd` a job execution?

=== [[createSource]] `createSource` Method

[source, scala]
----
createSource(metadataPath: String): Source
----

CAUTION: FIXME

=== [[createSink]] `createSink` Method

CAUTION: FIXME

==== [[sourceSchema]] `sourceSchema` Internal Method

[source, scala]
----
sourceSchema(): SourceInfo
----

`sourceSchema` returns the name and link:spark-sql-schema.adoc[schema] of the data source for streamed reading.

CAUTION: FIXME Why is the method called? Why does this bother with streamed reading and data sources?!

It supports two class hierarchies, i.e. link:spark-sql-FileFormat.adoc[FileFormat] and Structured Streaming's `StreamSourceProvider` data sources.

Internally, `sourceSchema` first creates an instance of the data source and...

CAUTION: FIXME Finish...

For Structured Streaming's `StreamSourceProvider` data sources, `sourceSchema` relays calls to `StreamSourceProvider.sourceSchema`.

For link:spark-sql-FileFormat.adoc[FileFormat] data sources, `sourceSchema` makes sure that `path` option was specified.

TIP: `path` is looked up in a case-insensitive way so `paTh` and `PATH` and `pAtH` are all acceptable. Use the lower-case version of `path`, though.

NOTE: `path` can use https://en.wikipedia.org/wiki/Glob_%28programming%29[glob pattern] (not regex syntax), i.e. contain any of `{}[]*?\` characters.

It checks whether the path exists if a glob pattern is not used. In case it did not exist you will see the following `AnalysisException` exception in the logs:

```
scala> spark.read.load("the.file.does.not.exist.parquet")
org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/jacek/dev/oss/spark/the.file.does.not.exist.parquet;
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:375)
  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$12.apply(DataSource.scala:364)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
  at scala.collection.immutable.List.foreach(List.scala:381)
  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
  at scala.collection.immutable.List.flatMap(List.scala:344)
  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:364)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:149)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:132)
  ... 48 elided
```

If link:spark-sql-properties.adoc#spark.sql.streaming.schemaInference[spark.sql.streaming.schemaInference] is disabled and the data source is different than link:spark-sql-TextFileFormat.adoc[TextFileFormat], and the input `userSpecifiedSchema` is not specified, the following `IllegalArgumentException` exception is thrown:

[options="wrap"]
----
Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.
----

CAUTION: FIXME I don't think the exception will ever happen for non-streaming sources since the schema is going to be defined earlier. When?

Eventually, it returns a `SourceInfo` with `FileSource[path]` and the schema (as calculated using the <<inferFileFormatSchema, inferFileFormatSchema>> internal method).

For any other data source, it throws `UnsupportedOperationException` exception:

```
Data source [className] does not support streamed reading
```

NOTE: `sourceSchema` is used exclusively when `DataSource` is requested for the <<sourceInfo, sourceInfo>>.

==== [[inferFileFormatSchema]] `inferFileFormatSchema` Internal Method

[source, scala]
----
inferFileFormatSchema(format: FileFormat): StructType
----

`inferFileFormatSchema` private method computes (aka _infers_) schema (as link:spark-sql-StructType.adoc[StructType]). It returns `userSpecifiedSchema` if specified or uses `FileFormat.inferSchema`. It throws a `AnalysisException` when is unable to infer schema.

It uses `path` option for the list of directory paths.

NOTE: It is used by <<sourceSchema, DataSource.sourceSchema>> and <<createSource, DataSource.createSource>> when link:spark-sql-FileFormat.adoc[FileFormat] is processed.

=== [[resolveRelation]] Resolving Relation (Creating BaseRelation) -- `resolveRelation` Method

[source, scala]
----
resolveRelation(checkFilesExist: Boolean = true): BaseRelation
----

`resolveRelation` resolves (i.e. creates) a link:spark-sql-BaseRelation.adoc[BaseRelation].

Internally, `resolveRelation` tries to create an instance of the <<providingClass, providingClass>> and branches off per its type and whether the optional <<userSpecifiedSchema, user-specified schema>> was specified or not.

.Resolving BaseRelation per Provider and User-Specified Schema
[cols="1,3",options="header",width="100%"]
|===
| Provider
| Behaviour

| link:spark-sql-SchemaRelationProvider.adoc[SchemaRelationProvider]
| Executes link:spark-sql-SchemaRelationProvider.adoc#createRelation[SchemaRelationProvider.createRelation] with the provided schema

| link:spark-sql-RelationProvider.adoc[RelationProvider]
| Executes link:spark-sql-RelationProvider.adoc#createRelation[RelationProvider.createRelation]

| link:spark-sql-FileFormat.adoc[FileFormat]
| Creates a link:spark-sql-BaseRelation.adoc#HadoopFsRelation[HadoopFsRelation]
|===

[NOTE]
====
`resolveRelation` is used when:

* `DataSource` is requested to <<writeAndRead, write and read>> the result of a structured query (only when <<providingClass, providingClass>> is a link:spark-sql-FileFormat.adoc[FileFormat])

* `DataFrameReader` is requested to link:spark-sql-DataFrameReader.adoc#load[load data from a data source that supports multiple paths]

* `TextInputCSVDataSource` and `TextInputJsonDataSource` are requested to infer schema

* `CreateDataSourceTableCommand` runnable command is link:spark-sql-LogicalPlan-CreateDataSourceTableCommand.adoc#run[executed]

* `CreateTempViewUsing` logical command is requested to <<spark-sql-LogicalPlan-CreateTempViewUsing.adoc#run, run>>

* `FindDataSourceTable` is requested to link:spark-sql-Analyzer-FindDataSourceTable.adoc#readDataSourceTable[readDataSourceTable]

* `ResolveSQLOnFile` is requested to convert a logical plan (when <<providingClass, providingClass>> is a link:spark-sql-FileFormat.adoc[FileFormat])

* `HiveMetastoreCatalog` is requested for link:spark-sql-HiveMetastoreCatalog.adoc#convertToLogicalRelation[convertToLogicalRelation]

* Structured Streaming's `FileStreamSource` creates batches of records
====

=== [[buildStorageFormatFromOptions]] `buildStorageFormatFromOptions` Method

[source, scala]
----
buildStorageFormatFromOptions(options: Map[String, String]): CatalogStorageFormat
----

`buildStorageFormatFromOptions`...FIXME

NOTE: `buildStorageFormatFromOptions` is used when...FIXME

=== [[creating-instance]][[apply]] Creating DataSource Instance

`DataSource` takes the following when created:

* [[sparkSession]] link:spark-sql-SparkSession.adoc[SparkSession]
* [[className]] Name of the provider class (aka _input data source format_)
* [[paths]] A list of file system paths that hold data (default: empty)
* [[userSpecifiedSchema]] (optional) User-specified link:spark-sql-StructType.adoc[schema] (default: `None`, i.e. undefined)
* [[partitionColumns]] (optional) Names of the partition columns (default: empty)
* [[bucketSpec]] Optional <<spark-sql-BucketSpec.adoc#, bucketing specification>> (default: `None`)
* [[options]] Options (default: empty)
* [[catalogTable]] (optional) link:spark-sql-CatalogTable.adoc[CatalogTable] (default: `None`)

`DataSource` initializes the <<internal-registries, internal registries and counters>>.

==== [[lookupDataSource]] Looking Up Class By Name Of Data Source Provider -- `lookupDataSource` Method

[source, scala]
----
lookupDataSource(
  provider: String,
  conf: SQLConf): Class[_]
----

`lookupDataSource` looks up the class name in the <<backwardCompatibilityMap, backwardCompatibilityMap>> and then replaces the class name exclusively for the `orc` provider per link:spark-sql-properties.adoc#spark.sql.orc.impl[spark.sql.orc.impl] internal configuration property:

* For `hive` (default), `lookupDataSource` uses `org.apache.spark.sql.hive.orc.OrcFileFormat`

* For `native`, `lookupDataSource` uses the canonical class name of link:spark-sql-OrcFileFormat.adoc[OrcFileFormat], i.e. `org.apache.spark.sql.execution.datasources.orc.OrcFileFormat`

With the provider's class name (aka _provider1_ internally) `lookupDataSource` assumes another name variant of format `[provider1].DefaultSource` (aka _provider2_ internally).

`lookupDataSource` then uses Java's link:++https://docs.oracle.com/javase/8/docs/api/java/util/ServiceLoader.html#load-java.lang.Class-java.lang.ClassLoader-++[ServiceLoader] to find all link:spark-sql-DataSourceRegister.adoc[DataSourceRegister] provider classes on the CLASSPATH.

`lookupDataSource` filters out the `DataSourceRegister` provider classes (by their link:spark-sql-DataSourceRegister.adoc#shortName[alias]) that match the _provider1_ (case-insensitive), e.g. `parquet` or `kafka`.

If a single provider class was found for the alias, `lookupDataSource` simply returns the provider class.

If no `DataSourceRegister` could be found by the short name (alias), `lookupDataSource` considers the names of the format provider as the fully-qualified class names and tries to load them instead (using Java's link:++https://docs.oracle.com/javase/8/docs/api/java/lang/ClassLoader.html#loadClass-java.lang.String-++[ClassLoader.loadClass]).

NOTE: You can reference your own custom `DataSource` in your code by link:spark-sql-DataFrameWriter.adoc#format[DataFrameWriter.format] method which is the alias or a fully-qualified class name.

CAUTION: FIXME Describe the other cases (orc and avro)

If no provider class could be found, `lookupDataSource` throws a `RuntimeException`:

[options="wrap"]
----
java.lang.ClassNotFoundException: Failed to find data source: [provider1]. Please find packages at http://spark.apache.org/third-party-projects.html
----

If however, `lookupDataSource` found multiple registered aliases for the provider name...FIXME

=== [[planForWriting]] Creating Logical Command for Writing (for CreatableRelationProvider and FileFormat Data Sources) -- `planForWriting` Method

[source, scala]
----
planForWriting(mode: SaveMode, data: LogicalPlan): LogicalPlan
----

`planForWriting` creates an instance of the <<providingClass, providingClass>> and branches off per its type as follows:

* For a <<spark-sql-CreatableRelationProvider.adoc#, CreatableRelationProvider>>, `planForWriting` creates a <<spark-sql-LogicalPlan-SaveIntoDataSourceCommand.adoc#creating-instance, SaveIntoDataSourceCommand>> (with the input `data` and `mode`, the `CreatableRelationProvider` data source and the <<caseInsensitiveOptions, caseInsensitiveOptions>>)

* For a <<spark-sql-FileFormat.adoc#, FileFormat>>, `planForWriting` <<planForWritingFileFormat, planForWritingFileFormat>> (with the `FileFormat` format and the input `mode` and `data`)

* For other types, `planForWriting` simply throws a `RuntimeException`:
+
```
[providingClass] does not allow create table as select.
```

[NOTE]
====
`planForWriting` is used when:

* `DataFrameWriter` is requested to <<spark-sql-DataFrameWriter.adoc#saveToV1Source, saveToV1Source>> (when `DataFrameWriter` is requested to <<spark-sql-DataFrameWriter.adoc#save, save the result of a structured query (a DataFrame) to a data source>> for <<spark-sql-DataSourceV2.adoc#, DataSourceV2>> with no `WriteSupport` and non-``DataSourceV2`` writers)

* <<spark-sql-LogicalPlan-InsertIntoDataSourceDirCommand.adoc#, InsertIntoDataSourceDirCommand>> logical command is executed
====

=== [[planForWritingFileFormat]] Planning for Writing (Using FileFormat) -- `planForWritingFileFormat` Internal Method

[source, scala]
----
planForWritingFileFormat(
  format: FileFormat,
  mode: SaveMode,
  data: LogicalPlan): InsertIntoHadoopFsRelationCommand
----

`planForWritingFileFormat` takes the <<paths, paths>> and the `path` option (from the <<caseInsensitiveOptions, caseInsensitiveOptions>>) together and (assuming that there is only one path available among the paths combined) creates a fully-qualified HDFS-compatible output path for writing.

NOTE: `planForWritingFileFormat` uses Hadoop HDFS's https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/Path.html[Path] to requests for the https://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html[FileSystem] that owns it (using <<spark-sql-SessionState.adoc#newHadoopConf, Hadoop Configuration>>).

`planForWritingFileFormat` uses the <<spark-sql-PartitioningUtils.adoc#, PartitioningUtils>> helper object to <<spark-sql-PartitioningUtils.adoc#validatePartitionColumn, validate partition columns>> in the <<partitionColumns, partitionColumns>>.

In the end, `planForWritingFileFormat` returns a new <<spark-sql-LogicalPlan-InsertIntoHadoopFsRelationCommand.adoc#, InsertIntoHadoopFsRelationCommand>>.

When the number of the <<paths, paths>> is different than `1`, `planForWritingFileFormat` throws an `IllegalArgumentException`:

```
Expected exactly one path to be specified, but got: [allPaths]
```

NOTE: `planForWritingFileFormat` is used when `DataSource` is requested to <<writeAndRead, write data to a data source per save mode followed by reading rows back>> (when <<spark-sql-LogicalPlan-CreateDataSourceTableAsSelectCommand.adoc#, CreateDataSourceTableAsSelectCommand>> logical command is executed) and for the <<planForWriting, logical command for writing>>.

=== [[getOrInferFileFormatSchema]] `getOrInferFileFormatSchema` Internal Method

[source, scala]
----
getOrInferFileFormatSchema(
  format: FileFormat,
  fileStatusCache: FileStatusCache = NoopCache): (StructType, StructType)
----

`getOrInferFileFormatSchema`...FIXME

NOTE: `getOrInferFileFormatSchema` is used when `DataSource` is requested for the <<sourceSchema, sourceSchema>> and to <<resolveRelation, resolveRelation>>.
