== [[HiveTableScanExec]] HiveTableScanExec Leaf Physical Operator

`HiveTableScanExec` is a <<spark-sql-SparkPlan.adoc#LeafExecNode, leaf physical operator>> that represents a <<spark-sql-LogicalPlan-HiveTableRelation.adoc#, HiveTableRelation>> logical operator at execution time.

`HiveTableScanExec` is <<creating-instance, created>> exclusively when <<spark-sql-SparkStrategy-HiveTableScans.adoc#, HiveTableScans>> execution planning strategy plans a <<spark-sql-LogicalPlan-HiveTableRelation.adoc#, HiveTableRelation>> logical operator (i.e. is executed on a logical query plan with a `HiveTableRelation` logical operator).

[[metrics]]
.HiveTableScanExec's Performance Metrics
[cols="1m,2,2",options="header",width="100%"]
|===
| Key
| Name (in web UI)
| Description

| numOutputRows
| number of output rows
| [[numOutputRows]]
|===

[[internal-registries]]
.HiveTableScanExec's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,2",options="header",width="100%"]
|===
| Name
| Description

| hiveQlTable
a| [[hiveQlTable]] Hive's `Table` metadata (<<spark-sql-HiveClientImpl.adoc#toHiveTable, converted>> from the <<spark-sql-LogicalPlan-HiveTableRelation.adoc#tableMeta, CatalogTable>> of the <<relation, HiveTableRelation>>)

Used when `HiveTableScanExec` is requested for the <<tableDesc, tableDesc>>, <<rawPartitions, rawPartitions>> and is <<doExecute, executed>>

| rawPartitions
a| [[rawPartitions]]

| tableDesc
a| [[tableDesc]] Hive's `TableDesc`
|===

=== [[creating-instance]] Creating HiveTableScanExec Instance

`HiveTableScanExec` takes the following when created:

* [[requestedAttributes]] Requested <<spark-sql-Expression-Attribute.adoc#, attributes>>
* [[relation]] <<spark-sql-LogicalPlan-HiveTableRelation.adoc#, HiveTableRelation>>
* [[partitionPruningPred]] Partition pruning predicate <<spark-sql-Expression.adoc#, expression>>
* [[sparkSession]] <<spark-sql-SparkSession.adoc#, SparkSession>>

`HiveTableScanExec` initializes the <<internal-registries, internal registries and counters>>.

=== [[doExecute]] Executing Physical Operator (Generating RDD[InternalRow]) -- `doExecute` Method

[source, scala]
----
doExecute(): RDD[InternalRow]
----

NOTE: `doExecute` is part of <<spark-sql-SparkPlan.adoc#doExecute, SparkPlan Contract>> to generate the runtime representation of a structured query as a distributed computation over <<spark-sql-InternalRow.adoc#, internal binary rows>> on Apache Spark (i.e. `RDD[InternalRow]`).

`doExecute`...FIXME
