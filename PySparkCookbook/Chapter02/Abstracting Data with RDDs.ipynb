{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Cookbook: RDDs\n",
    "## Tomasz Drabas, Denny Lee\n",
    "\n",
    "#### Version: 0.1\n",
    "\n",
    "#### 2018-07-01 (Happy Canada Day!)\n",
    "\n",
    "This notebook is in support of [PySpark Cookbook](): Chapter 2 on RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parallelize() missing 1 required positional argument: 'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4174538ef00e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m myRDD = sc.parallelize( \n\u001b[0;32m----> 7\u001b[0;31m  \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Amber'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Alfred'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Skye'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Albert'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Amber'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: parallelize() missing 1 required positional argument: 'c'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark-2.4.4')\n",
    "from pyspark import SparkConf, SparkContext as sc\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "myRDD = sc.parallelize( \n",
    " [('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12), ('Amber', 9)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myRDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3a0444f474bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myRDD' is not defined"
     ]
    }
   ],
   "source": [
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.textFile('/databricks-datasets/flights/airport-codes-na.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.textFile('/databricks-datasets/flights/airport-codes-na.txt').map(lambda line: line.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.textFile('/databricks-datasets/flights/airport-codes-na.txt', minPartitions=4, use_unicode=True).map(lambda line: line.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.textFile('/databricks-datasets/flights/departuredelays.csv').map(lambda line: line.split(\",\"))\n",
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.textFile('/databricks-datasets/flights/departuredelays.csv', minPartitions=8).map(lambda line: line.split(\",\"))\n",
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Using DataFrame*\n",
    "Note, that its faster (2.44s for DF, 2.96s for RDD w/ 8 partitions) while DF also takes into account of the header and can infer the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF = spark.read.csv('/databricks-datasets/flights/departuredelays.csv', header=True, inferSchema=True)\n",
    "myDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = sc.textFile('/databricks-datasets/flights/airport-codes-na.txt').map(lambda line: line.split(\"\\t\"))\n",
    "airports.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sc.textFile('/databricks-datasets/flights/departuredelays.csv').map(lambda line: line.split(\",\"))\n",
    "flights.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.map(lambda c: (c[0], c[1])).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.map(lambda c: (c[0], c[1])).filter(lambda c: c[1] == \"WA\").take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flatMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.filter(lambda c: c[1] == \"WA\").map(lambda c: (c[0], c[1])).flatMap(lambda x: x).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.map(lambda c: c[2]).distinct().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.map(lambda c: c[3]).sample(False, 0.001, 123).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leftOuterJoin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.map(lambda c: (c[3], c[0])).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.map(lambda c: (c[3], c[1])).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "flt = flights.map(lambda c: (c[3], c[0]))\n",
    "air = airports.map(lambda c: (c[3], c[1]))\n",
    "flt.join(air).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "flt = flights.map(lambda c: (c[3], c[0]))\n",
    "air = airports.map(lambda c: (c[3], c[1]))\n",
    "flt.join(air)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights2 = flights.repartition(8)\n",
    "flights2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapPartitionsWithIndex\n",
    "#flights.mapPartitionsWithIndex{ (idx, iter) => if (idx == 0) iter.drop(1) else iter }\n",
    "rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
    "def f(splitIndex, iterator): yield splitIndex\n",
    "rdd.mapPartitionsWithIndex(f).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View each row within RDD + the index \n",
    "# i.e. output is in form ([row], idx)\n",
    "ac = airports.map(lambda c: (c[0], c[3]))\n",
    "ac.zipWithIndex().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipWithIndex\n",
    "#   Skip header row by \n",
    "#   - filter out row 0\n",
    "#   - extract only row info\n",
    "ac.zipWithIndex()\\\n",
    "  .filter(lambda (row, idx): idx > 0)\\\n",
    "  .map(lambda (row, idx): row)\\\n",
    "  .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the origin code and delays, remove header\n",
    "# runs a group by origin code via reduceByKey()\n",
    "# sorting by the key (origin code)\n",
    "flights.zipWithIndex()\\\n",
    "  .filter(lambda (row, idx): idx > 0)\\\n",
    "  .map(lambda (row, idx): row)\\\n",
    "  .map(lambda c: (c[3], int(c[1])))\\\n",
    "  .reduceByKey(lambda x, y: x + y)\\\n",
    "  .sortByKey()\\\n",
    "  .take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create `a` RDD of Washington airports\n",
    "a = airports.zipWithIndex()\\\n",
    "  .filter(lambda (row, idx): idx > 0)\\\n",
    "  .map(lambda (row, idx): row)\\\n",
    "  .filter(lambda c: c[1] == \"WA\")\n",
    "\n",
    "# Create `b` RDD of British Columbia airports\n",
    "b = airports.zipWithIndex()\\\n",
    "  .filter(lambda (row, idx): idx > 0)\\\n",
    "  .map(lambda (row, idx): row)\\\n",
    "  .filter(lambda c: c[1] == \"BC\")\n",
    "\n",
    "# Union WA and BC airports\n",
    "a.union(b).take(50)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first RDD\n",
    "a = airports.zipWithIndex()\\\n",
    "  .filter(lambda (row, idx): idx > 0)\\\n",
    "  .map(lambda (row, idx): row)\\\n",
    "  .filter(lambda c: c[1] == \"WA\")\\\n",
    "  .map(lambda c: c[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.zipWithIndex()\\\n",
    "  .filter(lambda (row, idx): idx > 0)\\\n",
    "  .map(lambda (row, idx): row)\\\n",
    "  .take(50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.zipWithIndex()\\\n",
    "  .filter(lambda (row, idx): idx > 0)\\\n",
    "  .map(lambda (row, idx): row)\\\n",
    "  .filter(lambda c: c[1] == '0')\\\n",
    "  .take(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Actions\n",
    "\n",
    "Same Getting Ready as Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take(n)\n",
    "airports.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect()\n",
    "airports.filter(lambda c: c[1] == \"WA\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce(f)\n",
    "flights\\\n",
    "   .filter(lambda c: c[3] == 'SEA' and c[4] == 'SFO')\\\n",
    "   .map(lambda c: int(c[1]))\\\n",
    "   .reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceByKey\n",
    "#   Determine delays by originating city\n",
    "flights.zipWithIndex()\\\n",
    "  .filter(lambda (row, idx): idx > 0)\\\n",
    "  .map(lambda (row, idx): row)\\\n",
    "  .map(lambda c: (c[3], int(c[1])))\\\n",
    "  .reduceByKey(lambda x, y: x + y)\\\n",
    "  .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "flights.zipWithIndex()\\\n",
    "  .filter(lambda (row, idx): idx > 0)\\\n",
    "  .map(lambda (row, idx): row)\\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveAsTextFile\n",
    "airports.saveAsTextFile(\"/tmp/denny/airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs ls /tmp/denny/airports/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfalls of using RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting Ready\n",
    "flights = sc.textFile('/databricks-datasets/flights/departuredelays.csv')\\\n",
    "  .map(lambda line: line.split(\",\"))\\\n",
    "  .zipWithIndex()\\\n",
    "  .filter(lambda (row, idx): idx > 0)\\\n",
    "  .map(lambda (row, idx): row)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF = spark.read.options(header='true', inferSchema='true').csv('/databricks-datasets/flights/departuredelays.csv')\n",
    "flightsDF.createOrReplaceTempView(\"flightsDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to do it\n",
    "flights.map(lambda c: (c[3], int(c[1]))).reduceByKey(lambda x, y: x + y).sortByKey().take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select origin, sum(delay) as TotalDelay from flightsDF group by origin order by origin\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting Ready\n",
    "flights = sc.textFile('/databricks-datasets/flights/departuredelays.csv', minPartitions=8)\\\n",
    "  .map(lambda line: line.split(\",\"))\\\n",
    "  .zipWithIndex()\\\n",
    "  .filter(lambda (row, idx): idx > 0)\\\n",
    "  .map(lambda (row, idx): row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/a/38957067/1100699\n",
    "def count_in_a_partition(idx, iterator):\n",
    "  count = 0\n",
    "  for _ in iterator:\n",
    "    count += 1\n",
    "  return idx, count\n",
    "\n",
    "\n",
    "flights.mapPartitionsWithIndex(count_in_a_partition).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to do it\n",
    "flights.map(lambda c: (c[3], int(c[1]))).reduceByKey(lambda x, y: x + y).sortByKey().take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "RDDs",
  "notebookId": 1046754112728474
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
